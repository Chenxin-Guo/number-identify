{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/guochenxin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/guochenxin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/guochenxin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "import re\n",
    "from collections import Counter \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# doc2Vec package:\n",
    "import gensim.models as g\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>NPS/Ease</th>\n",
       "      <th>Good \\nBad \\nNeutral</th>\n",
       "      <th>Category</th>\n",
       "      <th>NPS Comment/Enhance</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>September</td>\n",
       "      <td>Passive</td>\n",
       "      <td>G</td>\n",
       "      <td>misc</td>\n",
       "      <td>Pretty OK</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>September</td>\n",
       "      <td>Promoter</td>\n",
       "      <td>G</td>\n",
       "      <td>product/industry</td>\n",
       "      <td>My advisor is very knowledgeable, uses the bes...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>September</td>\n",
       "      <td>Promoter</td>\n",
       "      <td>G</td>\n",
       "      <td>product/industry</td>\n",
       "      <td>I have a good agent here in Honolulu. I think ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>September</td>\n",
       "      <td>Promoter</td>\n",
       "      <td>G</td>\n",
       "      <td>product/industry</td>\n",
       "      <td>Very good product offering.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>September</td>\n",
       "      <td>Promoter</td>\n",
       "      <td>G</td>\n",
       "      <td>product/industry</td>\n",
       "      <td>JH has the best IUL's in the industry</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Month  NPS/Ease Good \\nBad \\nNeutral          Category  \\\n",
       "0  September   Passive                    G              misc   \n",
       "1  September  Promoter                    G  product/industry   \n",
       "2  September  Promoter                    G  product/industry   \n",
       "3  September  Promoter                    G  product/industry   \n",
       "4  September  Promoter                    G  product/industry   \n",
       "\n",
       "                                 NPS Comment/Enhance OTHER  \n",
       "0                                          Pretty OK   NaN  \n",
       "1  My advisor is very knowledgeable, uses the bes...   NaN  \n",
       "2  I have a good agent here in Honolulu. I think ...   NaN  \n",
       "3                        Very good product offering.   NaN  \n",
       "4              JH has the best IUL's in the industry   NaN  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = pd.read_excel('/Users/guochenxin/Desktop/Copy of inforce insurance portal comment sample.xlsx')\n",
    "comments.dropna(subset=['NPS Comment/Enhance', 'OTHER'],how = 'all', inplace = True)\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-c351d8f85220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NPS Comment/Enhance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4729\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4731\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4732\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 20"
     ]
    }
   ],
   "source": [
    "wordlist=[]\n",
    "for i in range(comments.shape[0]):\n",
    "    if comments['NPS Comment/Enhance'].isnull()[i] ==True:\n",
    "        continue\n",
    "    else:\n",
    "        pattern = '\\s|[,.!%-:]'\n",
    "        a = re.split(pattern, comments['NPS Comment/Enhance'][i])\n",
    "   \n",
    "        length = len(a)\n",
    "        j=0\n",
    "        while j < length:\n",
    "            if (a[j] == ''):\n",
    "                a.remove(a[j])\n",
    "                length = length - 1\n",
    "                continue\n",
    "            j = j+1\n",
    "        for word in a:\n",
    "            wordlist.append(word)\n",
    "print(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(comments.shape[0]):\n",
    "#     if comments['NPS Comment/Enhance'].isnull()[i] ==True:\n",
    "#         continue\n",
    "#     else:\n",
    "#         a = re.split(pattern, comments['NPS Comment/Enhance'][i])\n",
    "#         print(a)\n",
    "\n",
    "words = list(map(lambda x:x.lower(),wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pretty', 'ok', 'advisor', 'knowledgeable']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = [w for w in words if not w in stopwords] \n",
    "all_words[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['NPS Comment/Enhance'] = comments['NPS Comment/Enhance'].astype(str)\n",
    "comments['OTHER'] = comments['OTHER'].astype(str)\n",
    "texts1 = ' '.join([w for w in comments['NPS Comment/Enhance']])\n",
    "texts1 = texts1.lower()\n",
    "texts2 = ' '.join([w for w in comments['OTHER']])\n",
    "texts2 = texts2.lower()\n",
    "texts = texts1+' '+texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1051271d0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update(['nan','url',\"website\",'web','online', \"site\", \"information\", \"policy\", \"time\", 'account', 'company', 'page', 'login','access','make', 'insurance', \n",
    "                  'work','see','now','jh','able','t'])\n",
    "\n",
    "wordcloud = WordCloud(stopwords = stopwords, max_font_size=50, max_words=100, background_color=\"white\").generate(texts)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "wordcloud.to_file(\"first_review.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 5),\n",
       " ('best', 3),\n",
       " ('great', 3),\n",
       " ('service', 3),\n",
       " ('easy', 3),\n",
       " ('think', 2),\n",
       " ('customer', 2),\n",
       " ('e', 2),\n",
       " ('mailed', 2),\n",
       " ('pretty', 1)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "Counter = Counter(word for word in words if word not in stopwords) \n",
    "most_occur = Counter.most_common(10) \n",
    "most_occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence without removing stopwords\n",
    "sent = [word_tokenize(sent) for sent in sent_tokenize(texts)]\n",
    "\n",
    "# sentence remove stopwords:\n",
    "#stop_words = set(stopwords.words('english')) \n",
    "stop_words = set(STOPWORDS)\n",
    "stop_words.update([ \"information\",  'company', 'page', 'login', 'insurance', \n",
    "                  'now','jh','john', 'hancock'])\n",
    "\n",
    "sent_tokenize(texts)\n",
    "sent_list=[]\n",
    "for sentence in sent_tokenize(texts):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # get rid of punctuation\n",
    "    word_tokens = tokenizer.tokenize(sentence)\n",
    "   #word_tokens = word_tokenize(sentence)\n",
    "    words_set = [w for w in word_tokens if not w in stop_words] \n",
    "    sent_list.append(words_set)\n",
    "    \n",
    "model1 = gensim.models.Word2Vec(sent_list, min_count = 1, size = 100, window = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guochen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.039576553"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.similarity('missing', 'error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brought', 0.3613798916339874),\n",
       " ('show', 0.2987709045410156),\n",
       " ('paid', 0.2984021306037903),\n",
       " ('saved', 0.28848978877067566),\n",
       " ('quarterly', 0.27528831362724304),\n",
       " ('cases', 0.27164143323898315),\n",
       " ('despite', 0.26988911628723145),\n",
       " ('usually', 0.26644980907440186),\n",
       " ('couldn', 0.23408208787441254),\n",
       " ('hoping', 0.2305145561695099)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words =model1.wv.most_similar('recommend')\n",
    "sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guochen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4606204"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(sent_list, min_count = 2, size = 100, window = 5, sg = 1) \n",
    "model2.similarity('missing', 'error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('show', 0.6827774047851562),\n",
       " ('find', 0.6678767800331116),\n",
       " ('value', 0.6616883277893066),\n",
       " ('policies', 0.6598153710365295),\n",
       " ('website', 0.6586673259735107),\n",
       " ('premium', 0.6573193073272705),\n",
       " ('data', 0.6469952464103699),\n",
       " ('nan', 0.6466132998466492),\n",
       " ('nothing', 0.6434758901596069),\n",
       " ('policy', 0.6430102586746216)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words =model2.wv.most_similar('recommend')\n",
    "sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'web' website - Skip Gram :  0.93752456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guochen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\guochen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'web' \" + \"website - Skip Gram : \", model2.similarity('web', 'website')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding techniques (3 ways) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of words models: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(final_sent)\n",
    "# Show the Bag-of-Words Model as a pandas DataFrame\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "\n",
    "pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 830)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>addition</th>\n",
       "      <th>address</th>\n",
       "      <th>adequate</th>\n",
       "      <th>admit</th>\n",
       "      <th>advice</th>\n",
       "      <th>advisor</th>\n",
       "      <th>after</th>\n",
       "      <th>age</th>\n",
       "      <th>agent</th>\n",
       "      <th>aggregated</th>\n",
       "      <th>...</th>\n",
       "      <th>worst</th>\n",
       "      <th>would</th>\n",
       "      <th>wrong</th>\n",
       "      <th>xxx</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.283832</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.402064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 830 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   addition  address  adequate  admit  advice   advisor  after  age     agent  \\\n",
       "0       0.0      0.0       0.0    0.0     0.0  0.000000    0.0  0.0  0.000000   \n",
       "1       0.0      0.0       0.0    0.0     0.0  0.283832    0.0  0.0  0.000000   \n",
       "2       0.0      0.0       0.0    0.0     0.0  0.000000    0.0  0.0  0.402064   \n",
       "3       0.0      0.0       0.0    0.0     0.0  0.000000    0.0  0.0  0.000000   \n",
       "4       0.0      0.0       0.0    0.0     0.0  0.000000    0.0  0.0  0.000000   \n",
       "\n",
       "   aggregated  ...  worst  would  wrong  xxx  year  yes  yet  you  your  zero  \n",
       "0         0.0  ...    0.0    0.0    0.0  0.0   0.0  0.0  0.0  0.0   0.0   0.0  \n",
       "1         0.0  ...    0.0    0.0    0.0  0.0   0.0  0.0  0.0  0.0   0.0   0.0  \n",
       "2         0.0  ...    0.0    0.0    0.0  0.0   0.0  0.0  0.0  0.0   0.0   0.0  \n",
       "3         0.0  ...    0.0    0.0    0.0  0.0   0.0  0.0  0.0  0.0   0.0   0.0  \n",
       "4         0.0  ...    0.0    0.0    0.0  0.0   0.0  0.0  0.0  0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 830 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = comments['NPS Comment/Enhance'].tolist()\n",
    "sent_remove_punc=[]\n",
    "for sent in sent1:\n",
    "    pattern = r\"[^\\w]\"\n",
    "    sent_remove_punc.append(re.sub(pattern, \" \", str(sent))) # remove punctuation\n",
    "sent_remove_punc\n",
    "\n",
    "\n",
    "stop_words = stopwords\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "final_sent = []\n",
    "for sent in sent_remove_punc:\n",
    "    new_sent_words = []\n",
    "    word_token = word_tokenize(sent)\n",
    "    for w in word_token:\n",
    "        new_w = lemmatizer.lemmatize(w) # grouping together the different inflected forms of a word so they can be analysed as a single item\n",
    "        new_sent_words.append(new_w)\n",
    "    new_sent_word_list = [i for i in new_sent_words if not i in stop_words] # remove unnecessary stopwords\n",
    "    new_sent = ' '.join(new_sent_word_list)\n",
    "    final_sent.append(new_sent)\n",
    "final_sent # reduce columns number from 1319 to 837\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "values = tfidf.fit_transform(final_sent)\n",
    "feature_names = tfidf.get_feature_names()\n",
    "tfidf_model = pd.DataFrame(values.toarray(), columns = feature_names)\n",
    "tfidf_model = tfidf_model.iloc[:, 32:]\n",
    "print(tfidf_model.shape)\n",
    "tfidf_model.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2Vec ###\n",
    "### Please see the link for reference https://github.com/jhlau/doc2vec ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296017, 300)\n",
      "(296017, 300)\n"
     ]
    }
   ],
   "source": [
    "data1 = np.load('/Users/guochenxin/Desktop/apnews_dbow/doc2vec.bin.syn0.npy')\n",
    "print(data1.shape)\n",
    "data2 = np.load('/Users/guochenxin/Desktop/apnews_dbow/doc2vec.bin.syn1neg.npy')\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "2019-12-15 09:25:48,106 : INFO : collecting all words and their counts\n",
      "2019-12-15 09:25:48,108 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-12-15 09:25:48,111 : INFO : collected 951 word types and 288 unique tags from a corpus of 288 examples and 2320 words\n",
      "2019-12-15 09:25:48,112 : INFO : Loading a fresh vocabulary\n",
      "2019-12-15 09:25:48,114 : INFO : effective_min_count=1 retains 951 unique words (100% of original 951, drops 0)\n",
      "2019-12-15 09:25:48,115 : INFO : effective_min_count=1 leaves 2320 word corpus (100% of original 2320, drops 0)\n",
      "2019-12-15 09:25:48,125 : INFO : deleting the raw counts dictionary of 951 items\n",
      "2019-12-15 09:25:48,126 : INFO : sample=1e-05 downsamples 951 most-common words\n",
      "2019-12-15 09:25:48,126 : INFO : downsampling leaves estimated 222 word corpus (9.6% of prior 2320)\n",
      "2019-12-15 09:25:48,129 : INFO : estimated required memory for 951 words and 300 dimensions: 3103500 bytes\n",
      "2019-12-15 09:25:48,129 : INFO : resetting layer weights\n",
      "2019-12-15 09:25:48,161 : INFO : training model with 1 workers on 952 vocabulary and 300 features, using sg=1 hs=0 sample=1e-05 negative=5 window=15\n",
      "2019-12-15 09:25:48,177 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,178 : INFO : EPOCH - 1 : training on 2320 raw words (528 effective words) took 0.0s, 39749 effective words/s\n",
      "2019-12-15 09:25:48,191 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,192 : INFO : EPOCH - 2 : training on 2320 raw words (508 effective words) took 0.0s, 40402 effective words/s\n",
      "2019-12-15 09:25:48,203 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,204 : INFO : EPOCH - 3 : training on 2320 raw words (503 effective words) took 0.0s, 43831 effective words/s\n",
      "2019-12-15 09:25:48,214 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,215 : INFO : EPOCH - 4 : training on 2320 raw words (512 effective words) took 0.0s, 58223 effective words/s\n",
      "2019-12-15 09:25:48,227 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,228 : INFO : EPOCH - 5 : training on 2320 raw words (507 effective words) took 0.0s, 45062 effective words/s\n",
      "2019-12-15 09:25:48,240 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,241 : INFO : EPOCH - 6 : training on 2320 raw words (529 effective words) took 0.0s, 48084 effective words/s\n",
      "2019-12-15 09:25:48,252 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,254 : INFO : EPOCH - 7 : training on 2320 raw words (500 effective words) took 0.0s, 42615 effective words/s\n",
      "2019-12-15 09:25:48,264 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,265 : INFO : EPOCH - 8 : training on 2320 raw words (512 effective words) took 0.0s, 56867 effective words/s\n",
      "2019-12-15 09:25:48,278 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,278 : INFO : EPOCH - 9 : training on 2320 raw words (483 effective words) took 0.0s, 42272 effective words/s\n",
      "2019-12-15 09:25:48,291 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,293 : INFO : EPOCH - 10 : training on 2320 raw words (494 effective words) took 0.0s, 38602 effective words/s\n",
      "2019-12-15 09:25:48,305 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,306 : INFO : EPOCH - 11 : training on 2320 raw words (520 effective words) took 0.0s, 44237 effective words/s\n",
      "2019-12-15 09:25:48,318 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,319 : INFO : EPOCH - 12 : training on 2320 raw words (508 effective words) took 0.0s, 50262 effective words/s\n",
      "2019-12-15 09:25:48,329 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,330 : INFO : EPOCH - 13 : training on 2320 raw words (520 effective words) took 0.0s, 59655 effective words/s\n",
      "2019-12-15 09:25:48,343 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,344 : INFO : EPOCH - 14 : training on 2320 raw words (509 effective words) took 0.0s, 42534 effective words/s\n",
      "2019-12-15 09:25:48,355 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,357 : INFO : EPOCH - 15 : training on 2320 raw words (512 effective words) took 0.0s, 45466 effective words/s\n",
      "2019-12-15 09:25:48,367 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,368 : INFO : EPOCH - 16 : training on 2320 raw words (492 effective words) took 0.0s, 56598 effective words/s\n",
      "2019-12-15 09:25:48,378 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,378 : INFO : EPOCH - 17 : training on 2320 raw words (506 effective words) took 0.0s, 56074 effective words/s\n",
      "2019-12-15 09:25:48,390 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,391 : INFO : EPOCH - 18 : training on 2320 raw words (522 effective words) took 0.0s, 46587 effective words/s\n",
      "2019-12-15 09:25:48,400 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,401 : INFO : EPOCH - 19 : training on 2320 raw words (535 effective words) took 0.0s, 87661 effective words/s\n",
      "2019-12-15 09:25:48,417 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,418 : INFO : EPOCH - 20 : training on 2320 raw words (513 effective words) took 0.0s, 34961 effective words/s\n",
      "2019-12-15 09:25:48,429 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,430 : INFO : EPOCH - 21 : training on 2320 raw words (522 effective words) took 0.0s, 49264 effective words/s\n",
      "2019-12-15 09:25:48,439 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,440 : INFO : EPOCH - 22 : training on 2320 raw words (506 effective words) took 0.0s, 60661 effective words/s\n",
      "2019-12-15 09:25:48,458 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,460 : INFO : EPOCH - 23 : training on 2320 raw words (527 effective words) took 0.0s, 29191 effective words/s\n",
      "2019-12-15 09:25:48,471 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,472 : INFO : EPOCH - 24 : training on 2320 raw words (527 effective words) took 0.0s, 48956 effective words/s\n",
      "2019-12-15 09:25:48,486 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,486 : INFO : EPOCH - 25 : training on 2320 raw words (512 effective words) took 0.0s, 40195 effective words/s\n",
      "2019-12-15 09:25:48,499 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,500 : INFO : EPOCH - 26 : training on 2320 raw words (512 effective words) took 0.0s, 42732 effective words/s\n",
      "2019-12-15 09:25:48,511 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,513 : INFO : EPOCH - 27 : training on 2320 raw words (505 effective words) took 0.0s, 47987 effective words/s\n",
      "2019-12-15 09:25:48,523 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,523 : INFO : EPOCH - 28 : training on 2320 raw words (512 effective words) took 0.0s, 55823 effective words/s\n",
      "2019-12-15 09:25:48,534 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,534 : INFO : EPOCH - 29 : training on 2320 raw words (502 effective words) took 0.0s, 55397 effective words/s\n",
      "2019-12-15 09:25:48,546 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,547 : INFO : EPOCH - 30 : training on 2320 raw words (497 effective words) took 0.0s, 46953 effective words/s\n",
      "2019-12-15 09:25:48,557 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,558 : INFO : EPOCH - 31 : training on 2320 raw words (488 effective words) took 0.0s, 54488 effective words/s\n",
      "2019-12-15 09:25:48,569 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,570 : INFO : EPOCH - 32 : training on 2320 raw words (511 effective words) took 0.0s, 49478 effective words/s\n",
      "2019-12-15 09:25:48,581 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,582 : INFO : EPOCH - 33 : training on 2320 raw words (530 effective words) took 0.0s, 50657 effective words/s\n",
      "2019-12-15 09:25:48,592 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,594 : INFO : EPOCH - 34 : training on 2320 raw words (489 effective words) took 0.0s, 51653 effective words/s\n",
      "2019-12-15 09:25:48,603 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,604 : INFO : EPOCH - 35 : training on 2320 raw words (501 effective words) took 0.0s, 61787 effective words/s\n",
      "2019-12-15 09:25:48,614 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,616 : INFO : EPOCH - 36 : training on 2320 raw words (517 effective words) took 0.0s, 49870 effective words/s\n",
      "2019-12-15 09:25:48,627 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,628 : INFO : EPOCH - 37 : training on 2320 raw words (498 effective words) took 0.0s, 48437 effective words/s\n",
      "2019-12-15 09:25:48,638 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,638 : INFO : EPOCH - 38 : training on 2320 raw words (504 effective words) took 0.0s, 62873 effective words/s\n",
      "2019-12-15 09:25:48,648 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,649 : INFO : EPOCH - 39 : training on 2320 raw words (484 effective words) took 0.0s, 51532 effective words/s\n",
      "2019-12-15 09:25:48,660 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,661 : INFO : EPOCH - 40 : training on 2320 raw words (484 effective words) took 0.0s, 48740 effective words/s\n",
      "2019-12-15 09:25:48,670 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,671 : INFO : EPOCH - 41 : training on 2320 raw words (516 effective words) took 0.0s, 61422 effective words/s\n",
      "2019-12-15 09:25:48,680 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,682 : INFO : EPOCH - 42 : training on 2320 raw words (509 effective words) took 0.0s, 52267 effective words/s\n",
      "2019-12-15 09:25:48,691 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,692 : INFO : EPOCH - 43 : training on 2320 raw words (497 effective words) took 0.0s, 60864 effective words/s\n",
      "2019-12-15 09:25:48,702 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,703 : INFO : EPOCH - 44 : training on 2320 raw words (500 effective words) took 0.0s, 55108 effective words/s\n",
      "2019-12-15 09:25:48,712 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,714 : INFO : EPOCH - 45 : training on 2320 raw words (519 effective words) took 0.0s, 51775 effective words/s\n",
      "2019-12-15 09:25:48,726 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,726 : INFO : EPOCH - 46 : training on 2320 raw words (489 effective words) took 0.0s, 48295 effective words/s\n",
      "2019-12-15 09:25:48,736 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,737 : INFO : EPOCH - 47 : training on 2320 raw words (536 effective words) took 0.0s, 58799 effective words/s\n",
      "2019-12-15 09:25:48,747 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,748 : INFO : EPOCH - 48 : training on 2320 raw words (516 effective words) took 0.0s, 55554 effective words/s\n",
      "2019-12-15 09:25:48,757 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,758 : INFO : EPOCH - 49 : training on 2320 raw words (522 effective words) took 0.0s, 59605 effective words/s\n",
      "2019-12-15 09:25:48,767 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,768 : INFO : EPOCH - 50 : training on 2320 raw words (519 effective words) took 0.0s, 65261 effective words/s\n",
      "2019-12-15 09:25:48,778 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,779 : INFO : EPOCH - 51 : training on 2320 raw words (515 effective words) took 0.0s, 59504 effective words/s\n",
      "2019-12-15 09:25:48,788 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,789 : INFO : EPOCH - 52 : training on 2320 raw words (524 effective words) took 0.0s, 64682 effective words/s\n",
      "2019-12-15 09:25:48,801 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,801 : INFO : EPOCH - 53 : training on 2320 raw words (509 effective words) took 0.0s, 47288 effective words/s\n",
      "2019-12-15 09:25:48,812 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,812 : INFO : EPOCH - 54 : training on 2320 raw words (518 effective words) took 0.0s, 54713 effective words/s\n",
      "2019-12-15 09:25:48,822 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,822 : INFO : EPOCH - 55 : training on 2320 raw words (514 effective words) took 0.0s, 79661 effective words/s\n",
      "2019-12-15 09:25:48,831 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,832 : INFO : EPOCH - 56 : training on 2320 raw words (528 effective words) took 0.0s, 63052 effective words/s\n",
      "2019-12-15 09:25:48,840 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,841 : INFO : EPOCH - 57 : training on 2320 raw words (512 effective words) took 0.0s, 92960 effective words/s\n",
      "2019-12-15 09:25:48,850 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,851 : INFO : EPOCH - 58 : training on 2320 raw words (532 effective words) took 0.0s, 59479 effective words/s\n",
      "2019-12-15 09:25:48,861 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,862 : INFO : EPOCH - 59 : training on 2320 raw words (508 effective words) took 0.0s, 56152 effective words/s\n",
      "2019-12-15 09:25:48,871 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,871 : INFO : EPOCH - 60 : training on 2320 raw words (534 effective words) took 0.0s, 64260 effective words/s\n",
      "2019-12-15 09:25:48,881 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,882 : INFO : EPOCH - 61 : training on 2320 raw words (510 effective words) took 0.0s, 55753 effective words/s\n",
      "2019-12-15 09:25:48,891 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,892 : INFO : EPOCH - 62 : training on 2320 raw words (500 effective words) took 0.0s, 59842 effective words/s\n",
      "2019-12-15 09:25:48,901 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,902 : INFO : EPOCH - 63 : training on 2320 raw words (496 effective words) took 0.0s, 59931 effective words/s\n",
      "2019-12-15 09:25:48,912 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,912 : INFO : EPOCH - 64 : training on 2320 raw words (529 effective words) took 0.0s, 57864 effective words/s\n",
      "2019-12-15 09:25:48,921 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,922 : INFO : EPOCH - 65 : training on 2320 raw words (524 effective words) took 0.0s, 65377 effective words/s\n",
      "2019-12-15 09:25:48,931 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,932 : INFO : EPOCH - 66 : training on 2320 raw words (511 effective words) took 0.0s, 59553 effective words/s\n",
      "2019-12-15 09:25:48,941 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,941 : INFO : EPOCH - 67 : training on 2320 raw words (507 effective words) took 0.0s, 65834 effective words/s\n",
      "2019-12-15 09:25:48,951 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,952 : INFO : EPOCH - 68 : training on 2320 raw words (504 effective words) took 0.0s, 57873 effective words/s\n",
      "2019-12-15 09:25:48,961 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,962 : INFO : EPOCH - 69 : training on 2320 raw words (515 effective words) took 0.0s, 56943 effective words/s\n",
      "2019-12-15 09:25:48,971 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,972 : INFO : EPOCH - 70 : training on 2320 raw words (497 effective words) took 0.0s, 61414 effective words/s\n",
      "2019-12-15 09:25:48,981 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,982 : INFO : EPOCH - 71 : training on 2320 raw words (535 effective words) took 0.0s, 66769 effective words/s\n",
      "2019-12-15 09:25:48,991 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:48,992 : INFO : EPOCH - 72 : training on 2320 raw words (504 effective words) took 0.0s, 58195 effective words/s\n",
      "2019-12-15 09:25:49,001 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,002 : INFO : EPOCH - 73 : training on 2320 raw words (518 effective words) took 0.0s, 60437 effective words/s\n",
      "2019-12-15 09:25:49,010 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,011 : INFO : EPOCH - 74 : training on 2320 raw words (489 effective words) took 0.0s, 68620 effective words/s\n",
      "2019-12-15 09:25:49,021 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,021 : INFO : EPOCH - 75 : training on 2320 raw words (544 effective words) took 0.0s, 61009 effective words/s\n",
      "2019-12-15 09:25:49,030 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,031 : INFO : EPOCH - 76 : training on 2320 raw words (522 effective words) took 0.0s, 61675 effective words/s\n",
      "2019-12-15 09:25:49,040 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,040 : INFO : EPOCH - 77 : training on 2320 raw words (506 effective words) took 0.0s, 66399 effective words/s\n",
      "2019-12-15 09:25:49,050 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,050 : INFO : EPOCH - 78 : training on 2320 raw words (510 effective words) took 0.0s, 56960 effective words/s\n",
      "2019-12-15 09:25:49,060 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,061 : INFO : EPOCH - 79 : training on 2320 raw words (525 effective words) took 0.0s, 58268 effective words/s\n",
      "2019-12-15 09:25:49,070 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,071 : INFO : EPOCH - 80 : training on 2320 raw words (510 effective words) took 0.0s, 65186 effective words/s\n",
      "2019-12-15 09:25:49,080 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,081 : INFO : EPOCH - 81 : training on 2320 raw words (504 effective words) took 0.0s, 57619 effective words/s\n",
      "2019-12-15 09:25:49,090 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,090 : INFO : EPOCH - 82 : training on 2320 raw words (505 effective words) took 0.0s, 62257 effective words/s\n",
      "2019-12-15 09:25:49,100 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,101 : INFO : EPOCH - 83 : training on 2320 raw words (501 effective words) took 0.0s, 55821 effective words/s\n",
      "2019-12-15 09:25:49,110 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,111 : INFO : EPOCH - 84 : training on 2320 raw words (522 effective words) took 0.0s, 59200 effective words/s\n",
      "2019-12-15 09:25:49,121 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,121 : INFO : EPOCH - 85 : training on 2320 raw words (499 effective words) took 0.0s, 58174 effective words/s\n",
      "2019-12-15 09:25:49,130 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,130 : INFO : EPOCH - 86 : training on 2320 raw words (505 effective words) took 0.0s, 66949 effective words/s\n",
      "2019-12-15 09:25:49,140 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,140 : INFO : EPOCH - 87 : training on 2320 raw words (522 effective words) took 0.0s, 57464 effective words/s\n",
      "2019-12-15 09:25:49,150 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,150 : INFO : EPOCH - 88 : training on 2320 raw words (514 effective words) took 0.0s, 62093 effective words/s\n",
      "2019-12-15 09:25:49,160 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,161 : INFO : EPOCH - 89 : training on 2320 raw words (524 effective words) took 0.0s, 57057 effective words/s\n",
      "2019-12-15 09:25:49,170 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,171 : INFO : EPOCH - 90 : training on 2320 raw words (508 effective words) took 0.0s, 59417 effective words/s\n",
      "2019-12-15 09:25:49,180 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,180 : INFO : EPOCH - 91 : training on 2320 raw words (516 effective words) took 0.0s, 65003 effective words/s\n",
      "2019-12-15 09:25:49,188 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,189 : INFO : EPOCH - 92 : training on 2320 raw words (512 effective words) took 0.0s, 70879 effective words/s\n",
      "2019-12-15 09:25:49,198 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,199 : INFO : EPOCH - 93 : training on 2320 raw words (517 effective words) took 0.0s, 81250 effective words/s\n",
      "2019-12-15 09:25:49,207 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,208 : INFO : EPOCH - 94 : training on 2320 raw words (507 effective words) took 0.0s, 68486 effective words/s\n",
      "2019-12-15 09:25:49,217 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,217 : INFO : EPOCH - 95 : training on 2320 raw words (505 effective words) took 0.0s, 63646 effective words/s\n",
      "2019-12-15 09:25:49,226 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,226 : INFO : EPOCH - 96 : training on 2320 raw words (508 effective words) took 0.0s, 89230 effective words/s\n",
      "2019-12-15 09:25:49,235 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,235 : INFO : EPOCH - 97 : training on 2320 raw words (521 effective words) took 0.0s, 94709 effective words/s\n",
      "2019-12-15 09:25:49,244 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,244 : INFO : EPOCH - 98 : training on 2320 raw words (515 effective words) took 0.0s, 92118 effective words/s\n",
      "2019-12-15 09:25:49,254 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,254 : INFO : EPOCH - 99 : training on 2320 raw words (532 effective words) took 0.0s, 66710 effective words/s\n",
      "2019-12-15 09:25:49,265 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-15 09:25:49,265 : INFO : EPOCH - 100 : training on 2320 raw words (533 effective words) took 0.0s, 55319 effective words/s\n",
      "2019-12-15 09:25:49,266 : INFO : training on a 232000 raw words (51184 effective words) took 1.1s, 46347 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# wrote preprocessed text list into txt file\n",
    "File_object = open(r\"/Users/guochenxin/Desktop/train_texts.txt\",\"w+\")\n",
    "for i in final_sent:\n",
    "    File_object.writelines(i)\n",
    "    File_object.write('\\n')\n",
    "File_object.close()\n",
    "\n",
    "import gensim.models as g\n",
    "import logging\n",
    "\n",
    "#doc2vec parameters\n",
    "vector_size = 300\n",
    "window_size = 15\n",
    "min_count = 1\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 0 #0 = dbow; 1 = dmpv\n",
    "worker_count = 1 #number of parallel processes\n",
    "\n",
    "#pretrained word embeddings\n",
    "pretrained_emb = \"/Users/guochenxin/Documents/GitHub/doc2vec/toy_datatoy_data/pretrained_word_embeddings.txt\" \n",
    "#None if use without pretrained embeddings\n",
    "\n",
    "#input corpus\n",
    "train_corpus = \"/Users/guochenxin/Desktop/train_texts.txt\"\n",
    "\n",
    "# output model\n",
    "# saved_path = \"/Users/guochenxin/Desktop/model.bin\"\n",
    "\n",
    "#enable logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#train doc2vec model\n",
    "docs = g.doc2vec.TaggedLineDocument(train_corpus)\n",
    "model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, \n",
    "                  sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, \n",
    "                  dbow_words=1, dm_concat=1, pretrained_emb=pretrained_emb, iter=train_epoch)\n",
    "\n",
    "# save model\n",
    "# model.save(saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vectors\n",
    "model.docvecs[287].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Text Classification Modelling: 3 ways #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Recurrent Neural Network (RNN) using the Long Short Term Memory (LSTM) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
