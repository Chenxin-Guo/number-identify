#!/usr/bin/env python
# coding: utf-8

# <h2>Project 4: Empirical Risk Minimization</h2>
# 
# <blockquote>
#     <center>
#     <img src="./spam.jpeg" width="200px" />
#     </center>
#       <p><cite><center>"One person's spam is another person's dinner."<br>
#        -- ancient German wisdon
#       </center></cite></p>
# </blockquote>
# 
# <h3>Introduction</h3>
# 
# <p>
#     In this project you will be building an email spam filter.</p>
# 
# <strong>How to submit:</strong> You can submit your code using the red <strong>Submit</strong> button above. This button will send any code below surrounded by <strong>#&lt;GRADED&gt;</strong><strong>#&lt;/GRADED&gt;</strong> tags below to the autograder, which will then run several tests over your code. By clicking on the <strong>Details</strong> dropdown next to the Submit button, you will be able to view your submission report once the autograder has completed running. This submission report contains a summary of the tests you have failed or passed, as well as a log of any errors generated by your code when we ran it.
# 
# Note that this may take a while depending on how long your code takes to run! Once your code is submitted you may navigate away from the page as you desire -- the most recent submission report will always be available from the Details menu.
# 
# <p><strong>Evaluation:</strong> Your code will be autograded for technical
# correctness and--on some assignments--speed. Please <em>do not</em> change the names of any provided functions or classes within the code, or you will wreak havoc on the autograder. Furthermore, <em>any code not surrounded by <strong>#&lt;GRADED&gt;</strong><strong>#&lt;/GRADED&gt;</strong> tags will not be run by the autograder</em>. However, the correctness of your implementation -- not the autograder's output -- will be the final judge of your score.  If necessary, we will review and grade assignments individually to ensure that you receive due credit for your work.
# 
# <p><strong>Academic Integrity:</strong> We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else's code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don't try. We trust you all to submit your own work only; <em>please</em> don't let us down. If you do, we will pursue the strongest consequences available to us.
# 
# <p><strong>Getting Help:</strong> You are not alone!  If you find yourself stuck  on something, contact the course staff for help.  Office hours, section, and the <a href="https://piazza.com/class/jcb1ar25kjd5vq">Piazza</a> are there for your support; please use them.  If you can't make our office hours, let us know and we will schedule more.  We want these projects to be rewarding and instructional, not frustrating and demoralizing.  But, we don't know when or how to help unless you ask.
# 
# 
# 

# <h3>Computing derivatives</h3>
# 
# <p>  Before you dive into the programming part of this assignment you will need to derive the gradients for several loss functions. 
#     <b>Please write your calculation in a comment block within your code.</b> 
# </p>
# 
# <p>   Derive the gradient function for each of the following loss functions with respect to the weight vector $w$. Write down the gradient update (with stepsize $c$). <br>
# (Note that:    $\|w\|_2^2=w^\top w$ and  $\lambda$ is a  non-negative constant.)
# </p>
# 
# <ol>
#     <li> Ridge Regression: ${\cal L}(w)=\frac{1}{n}\sum_{i=1}^n (w^\top x_i-y_i)^2+\lambda \|w\|_2^2$ </li>
#     <li> Logistic Regression: ($y_i\in\{+1,-1\}$): ${\cal L}(w)=\sum_{i=1}^n \log(1+\exp{(-y_i w^\top x_i)})$ </li>
#     <li> Hinge loss: ($y_i\in\{+1,-1\}$): ${\cal L}(w)=\sum_{i=1}^n \max \left(1-y_i(w^\top x_i+b),0\right)+\lambda \|w\|_2^2$ </li>
# </ol>  

#  Ridge Regression gradient: $$\ g(w)=\frac{2}{n}\sum_{i=1}^{n}x_i^T(wx_i-y_i)+2\lambda w$$
# gradient update: $w_{t+1}=w_t-cg(w_t)$

#  Logistic Regression gradient: $$g(w)=\sum_{i=1}^{n}\frac{-y_ix_i}{1+e^{y_iw^Tx_i}} $$
#     gradient update: $w_{t+1}=w_t-cg(w_t)$

# Hing loss gradient: 
# \begin{equation}
# g(w)=\left\{
# \begin{array}{rcl}
# \sum_{i=1}^{n} max(1-y_i(w^Tx_i+b),0)'+2\lambda w 
# \end{array} \right.
# \end{equation}
# 
# gradient update: $w_{t+1}=w_t-cg(w_t)$

# <h3>Building an email spam filter</h3>
# <p> You will now implement ridge loss and the Adagrad algorithm.
#    
# The function below loads in pre-processed email data, where emails are represented as bag-of-words vectors.
# </p>
# 

# In[1]:


#<GRADED>
import numpy as np
#</GRADED>
import matplotlib
matplotlib.use('PDF')
import matplotlib.pyplot as plt
from numpy.matlib import repmat
import sys
from scipy.io import loadmat
import time

get_ipython().run_line_magic('matplotlib', 'inline')


# In[2]:


# tokenize the email and hashes the symbols into a vector
def extractfeaturesnaive(path, B):
    with open(path, 'r') as femail:
        # initialize all-zeros feature vector
        v = np.zeros(B)
        email = femail.read()
        # breaks for non-ascii characters
        tokens = email.split()
        for token in tokens:
            v[hash(token) % B] = 1
    return v

def loadspamdata(extractfeatures, B=512, path="../resource/lib/public/data_train/"):
    '''
    INPUT:
    extractfeatures : function to extract features
    B               : dimensionality of feature space
    path            : the path of folder to be processed
    
    OUTPUT:
    X, Y
    '''
    if path[-1] != '/':
        path += '/'
    
    with open(path + 'index', 'r') as f:
        allemails = [x for x in f.read().split('\n') if ' ' in x]
    
    xs = np.zeros((len(allemails), B))
    ys = np.zeros(len(allemails))
    for i, line in enumerate(allemails):
        label, filename = line.split(' ')
        # make labels +1 for "spam" and -1 for "ham"
        ys[i] = (label == 'spam') * 2 - 1
        xs[i, :] = extractfeatures(path + filename, B)
    print('Loaded %d input emails.' % len(ys))
    return xs, ys

X,Y = loadspamdata(extractfeaturesnaive)
X.shape


# This is your training set. To evaluate your algorithm you should split it off into a validation set.

# In[3]:


# Split data into training and validation
n, d = X.shape
cutoff = int(np.ceil(0.8 * n))
# indices of training samples
xTr = X[:cutoff,:]
yTr = Y[:cutoff]
# indices of testing samples
xTv = X[cutoff:,:]
yTv = Y[cutoff:]


# <p>This should generate a training data set <code>xTr</code>, <code>yTr</code> and a validation set <code>xTv</code>, <code>yTv</code> for you. </p>
# 
# <p>It is now time to implement your classifiers. We will always use the Adagrad gradient descent algorithm, but with various loss functions. 
# First implement the function <code>ridge</code> which computes the ridge regression loss and gradient for a particular data set <code>xTr</code>, <code>yTr</code> and a weight vector <code>w</code>. Make sure you don't forget to incorporate your regularization constant $\lambda$. </p>

# In[4]:


#<GRADED>
def ridge(w,xTr,yTr,lmbda):
    """
    INPUT:
    w     : d   dimensional weight vector
    xTr   : nxd dimensional matrix (each row is an input vector)
    yTr   : n   dimensional vector (each entry is a label)
    lmbda : regression constant (scalar)
    
    OUTPUTS:
    loss     : the total loss obtained with w on xTr and yTr (scalar)
    gradient : d dimensional gradient at w
    """
    n, d = xTr.shape
    
    
    s = w @ xTr.T - yTr## 1*n
         
    loss = (1 / n) * np.sum(s**2) + lmbda * w @ w.T
    gradient = 2 / n * (xTr.T @ s).T + 2 * lmbda * w


    return loss, gradient


    
#</GRADED>


# <p>An  alternative to  deriving the gradient analytically is to estimate it numerically. This is very slow, but it is a convenient  way to check your code for correctness.  The following function  uses numerical differentiation to evaluate the correctness of ridge.  If your code is correct, the norm difference between the two should be very small (smaller than $10^{-8}$). 
# Keep in mind that this only checks if the gradient corresponds to the loss, but not if the loss is correct. The function also plots an image of the gradient values (blue) and their estimates (red). If everything is correct, these two should be right on top of each other.
# </p>

# In[5]:


def numericalgradient(fun,w,e):
    # get dimensionality
    d = len(w)
    # initialize numerical derivative
    dh = np.zeros(d)
    # go through dimensions
    for i in range(d):
        # copy the weight vector
        nw = w.copy()
        # perturb dimension i
        nw[i] += e
        # compute loss
        l1, temp = fun(nw)
        # perturb dimension i again
        nw[i] -= 2*e
        # compute loss
        l2, temp = fun(nw)
        # the gradient is the slope of the loss
        dh[i] = (l1 - l2) / (2*e)
    return dh

def checkgrad(fun,w,e):
    # evaluate symbolic gradient from fun()
    loss,dy = fun(w)
    # estimate gradient numerically from fun()
    dh = numericalgradient(fun,w,e)
    
    # ii = dy.argsort()
    ii = np.array([i for i in range(len(dy))])
    
    plt.figure(figsize=(10,6))
    plt.scatter([i for i in range(len(dy))], dh[ii], c='b', marker='o', s=60)
    plt.scatter([i for i in range(len(dy))], dy[ii], c='r', marker='.', s=50)
    plt.xlabel('Dimension')
    plt.ylabel('Gradient value')
    plt.legend(["numeric","symbolic"])
    
    # return the norm of the difference scaled by the norm of the sum
    return np.linalg.norm(dh - dy) / np.linalg.norm(dh + dy)

# set lmbda (Î») arbitrarily
lmbda = 0.1
# dimensionality of the input
_, d = xTr.shape
# evaluate loss on random vector
w = np.random.rand(d)
# the lambda function notation is an inline way to define a function with only a single argument.
ratio = checkgrad(lambda weight: ridge(weight,xTr,yTr,lmbda),w,1e-05)
print("The norm ratio is %.10f." % ratio)


# <p>Implement the function <code>adagrad</code> which performs adaptive gradient descent. 
# Make sure to include the tolerance variable to stop early if the norm of the gradient is less than the tolerance value (you can use the function <code>np.linalg.norm(x)</code>). When the norm of the gradient is tiny it means that you have arrived at a minimum.  <br>
# The first parameter of <code>adagrad</code> is a function which takes a weight vector and returns loss and gradient.
# </p>                

# In[6]:


#<GRADED>
def adagrad(func,w,alpha,maxiter,eps,delta=1e-02):
    """
    INPUT:
    func    : function to minimize
              (loss, gradient = func(w))
    w       : d dimensional initial weight vector 
    alpha   : initial gradient descent stepsize (scalar)
    maxiter : maximum amount of iterations (scalar)
    eps     : epsilon value
    delta   : if norm(gradient)<delta, it quits (scalar)
    
    OUTPUTS:
     
    w      : d dimensional final weight vector
    losses : vector containing loss at each iteration
    """
    
    losses = np.zeros(maxiter)
    d = len(w)
    
    
    loss,gradient = func(w)
    i=0
    z=np.zeros(d)
    s=np.linalg.norm( alpha * gradient / np.sqrt(z + eps))
    while (s>delta and i<maxiter):
        z += gradient ** 2
        w -= alpha * gradient / np.sqrt(z + eps)
        s=np.linalg.norm( alpha * gradient / np.sqrt(z + eps))
        loss,gradient = func(w)
        losses[i] = loss
        i += 1
        
        
    return w , losses[0:i]
#</GRADED>


# In[7]:


_, d = xTr.shape
eps = 1e-06
w, losses = adagrad(lambda weight: ridge(weight, xTr, yTr, lmbda), np.random.rand(d), 1, 1000, eps)

plt.figure(figsize=(10,6))
plt.semilogy(losses, c='r', linestyle='-')
plt.xlabel("gradient updates")
plt.ylabel("loss")
plt.title("Adagrad convergence")
print("Final loss: %f" % losses[-1])


# <p> Write the (almost trivial) function <code>linclassify</code> which returns the predictions for a vector <code>w</code> and a data set <code>xTv</code>. (You can take it from a previous project.)</p>
# 
# <p>After this you can check your training and validation accuracy by running the cell below.</p>

# In[8]:


#<GRADED>
def linclassify(w,xTr):
    ## fill in your code here
    preds = np.sign(w @ xTr.T)
    return preds
#</GRADED>

# evaluate training accuracy
preds = linclassify(w,xTr)
trainingacc = np.mean(preds==yTr)
# evaluate testing accuracy
preds = linclassify(w,xTv)
validationacc = np.mean(preds==yTv)
print("Training accuracy %2.2f%%\nValidation accuracy %2.2f%%\n" % (trainingacc*100,validationacc*100))


# <p>Now implement the two other loss functions, <code>logistic</code> and <code>hinge</code>. Start off with <code>logistic</code>:</p>

# In[9]:


#<GRADED>
def logistic(w,xTr,yTr):
    """
    INPUT:
    w     : d   dimensional weight vector
    xTr   : nxd dimensional matrix (each row is an input vector)
    yTr   : n   dimensional vector (each entry is a label)
    
    OUTPUTS:
    loss     : the total loss obtained with w on xTr and yTr (scalar)
    gradient : d dimensional gradient at w
    """
    n, d = xTr.shape
    
    ## fill in your code here

    loss = 0
    g=0

    gradient = np.zeros(d)
    
    #####you du ba
    for i in range(n):
        ee = -yTr[i] * np.dot(w,xTr[i].T)
        loss += np.log(1 + np.exp(ee))
        gradient += -yTr[i] * xTr[i] / (1 + np.exp(-ee))

    return loss , gradient

#</GRADED>


# <p>You can use the two cells below to test how well this loss function performs.</p>

# In[10]:


# Gradient sanity check
_, d = xTr.shape
w = np.random.rand(d)
ratio = checkgrad(lambda weight: logistic(weight,xTr,yTr),w,1e-05)
print("The norm ratio is %.10f." % ratio)


# In[11]:


w, losses = adagrad(lambda weight: logistic(weight, xTr, yTr), np.random.rand(d), 1, 1000, 1e-06)

# evaluate training accuracy
preds = linclassify(w,xTr)
trainingacc = np.mean(preds==yTr)
# evaluate testing accuracy
preds = linclassify(w,xTv)
validationacc = np.mean(preds==yTv)
print("Training accuracy %2.2f%%\nValidation accuracy %2.2f%%\n" % (trainingacc*100,validationacc*100))


# <p>Now implement <code>hinge</code>:</p>

# In[24]:


#<GRADED>
def hinge(w,xTr,yTr,lmbda):
    """
    INPUT:
    w     : d   dimensional weight vector
    xTr   : nxd dimensional matrix (each row is an input vector)
    yTr   : n   dimensional vector (each entry is a label)
    lmbda : regression constant (scalar)
    
    OUTPUTS:
    loss     : the total loss obtained with w on xTr and yTr (scalar)
    gradient : d dimensional gradient at w
    """
    n, d = xTr.shape
    
    ## fill in your code here

    
    s = 1.0 - yTr * ( xTr @ w)
    ind = s<=0
    a = s
    a[ind] = 0
    loss = np.sum(a) + lmbda * np.dot(w, w)
    
    g = -yTr.reshape(n,1) * xTr
    g[ind] = 0
    gradient = np.sum(g , axis = 0) + 2 * lmbda * w

  
    
    return loss , gradient
#</GRADED>


# <p>You can use the two cells below to test how well this loss function performs.</p>

# In[21]:


# Gradient sanity check
lmbda = 0.1
_, d = xTr.shape
w = np.random.rand(d)
ratio = checkgrad(lambda weight: hinge(weight,xTr,yTr,lmbda),w,1e-05)
print("The norm ratio is %.10f." % ratio)


# In[22]:


w, losses = adagrad(lambda weight: hinge(weight, xTr, yTr, lmbda), np.random.rand(d), 1, 1000, 1e-06)

# evaluate training accuracy
preds = linclassify(w,xTr)
trainingacc = np.mean(preds==yTr)
# evaluate testing accuracy
preds = linclassify(w,xTv)
validationacc = np.mean(preds==yTv)
print("Training accuracy %2.2f%%\nValidation accuracy %2.2f%%\n" % (trainingacc*100,validationacc*100))


# <h3>Competition <b>(Optional)</b></h3>
# 
# <p>The competition for this assignment is split into two components:</p>
# 
# <ol>
# <li><b>Feature Extraction</b>:
# Modify the function <code>extractfeaturescomp</code>.
# This function takes in a file path <code>path</code> and
# a feature dimension <code>B</code> and should output a feature vector of dimension <code>B</code>.
# The autograder will pass in a file path pointing to a file that contains an email,
# and set <code>B</code> = <code>feature_dimension</code>.
# We provide <code>extractfeaturesnaive</code> as an example.
# </li>
# <li><b>Model Training</b>:
# Modify the function <code>trainspamfiltercomp</code>.
# This function takes in training data <code>xTr</code> and training labels <code>yTr</code> and
# should output a weight vector <code>w</code> for linear classification.
# We provide an initial implementation using Adagrad and ridge regression.
# </li>
# </ol>
# 
# <p>Your model will be trained on the same training set above (loaded by <code>loadspamdata</code>), but we will test its accuracy on a secret dataset of emails.</p>

# In[15]:


#<GRADED>
feature_dimension = 512
def extractfeaturescomp(path, B):
    '''
    INPUT:
    path : file path of email
    B    : dimensionality of feature vector
    
    OUTPUTS:
    x    : B dimensional vector
    '''
    x = np.zeros(B)
    with open(path, 'r') as femail:
        email = femail.read()
        # breaks for non-ascii characters
        tokens = email.split()
        for token in tokens:
            x[hash(token) % B] = 1
    return x
#</GRADED>


# In[16]:


#<GRADED>
def trainspamfiltercomp(xTr, yTr):
    '''
    INPUT:
    xTr : nxd dimensional matrix (each row is an input vector)
    yTr : d   dimensional vector (each entry is a label)
    
    OUTPUTS:
    w : d dimensional vector for linear classification
    '''
    w = np.random.rand(np.shape(xTr)[1])
    return w
#</GRADED>

